{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard\n",
    "!pip install -qqq accelerate==0.28.0\n",
    "!pip install -qqq transformers==4.48.3\n",
    "!pip install -qqq datasets==3.6.0\n",
    "\n",
    "!pip install -U accelerate\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_file = \"/content/drive/MyDrive/ai_term/train.zip\"\n",
    "extract_dir = \"data\"\n",
    "train_data_labels_path = os.path.join(extract_dir, \"train\", \"train_labels.csv\") \n",
    "\n",
    "if not os.path.exists(extract_dir) or not os.path.exists(train_data_labels_path):\n",
    "    print(f\"{extract_dir} 디렉토리 또는 필요한 파일({train_data_labels_path})이 존재하지 않습니다. 압축 해제합니다.\")\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "    !unzip -q {zip_file} -d {extract_dir}\n",
    "else:\n",
    "    print(f\"{extract_dir} 디렉토리와 필요한 파일이 이미 존재합니다. 압축 해제를 건너뜁니다.\")\n",
    "\n",
    "train_data_labels = train_data_labels_path\n",
    "train_image_path = os.path.join(extract_dir, \"train\", \"images\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_labels = \"./data/train/train_labels.csv\"\n",
    "train_image_path = \"./data/train/images/\"\n",
    "\n",
    "model_output_path = \"./output\"\n",
    "\n",
    "fruit_labels = [\"apple\", \"asian pear\", \"banana\", \"cherry\", \"grape\", \"pineapple\"]\n",
    "style_labels = [\"pencil color\", \"oil painting\", \"water color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import ViTModel, PreTrainedModel, ViTConfig\n",
    "import transformers as tf\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets.features import ClassLabel, Image\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.mps.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df['image'] = train_image_path + df['file_name']\n",
    "\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds = ds.cast_column(\"image\", Image())\n",
    "\n",
    "    return ds\n",
    "\n",
    "df = pd.read_csv(train_data_labels)\n",
    "\n",
    "dataset = load_data(train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "def preprocess_images(examples):\n",
    "    images = [feature_extractor(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    examples['pixel_values'] = [image['pixel_values'][0] for image in images]\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(preprocess_images, batched=True)\n",
    "dataset.set_format(type='torch', columns=['image', 'pixel_values', 'fruit', 'style'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "train_val = train_val_dataset[\"train\"]\n",
    "test = train_val_dataset[\"test\"]\n",
    "\n",
    "train_val = train_val.train_test_split(test_size=0.2, seed=42)\n",
    "train = train_val[\"train\"]\n",
    "val = train_val[\"test\"]\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskViTConfig(ViTConfig):\n",
    "    def __init__(self, num_fruit=6, num_style=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_fruit = num_fruit\n",
    "        self.num_style = num_style\n",
    "\n",
    "class MultiTaskViT(PreTrainedModel):\n",
    "    config_class = MultiTaskViTConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        hidden_size = self.vit.config.hidden_size\n",
    "\n",
    "        self.fruit_classifier = nn.Linear(hidden_size, config.num_fruit)\n",
    "        self.style_classifier = nn.Linear(hidden_size, config.num_style)\n",
    "\n",
    "    def forward(self, pixel_values, fruit=None, style=None, **kwargs):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output \n",
    "\n",
    "\n",
    "        fruit_logits = self.fruit_classifier(pooled_output)\n",
    "        style_logits = self.style_classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if fruit is not None and style is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            fruit_loss = loss_fn(fruit_logits, fruit)\n",
    "            style_loss = loss_fn(style_logits, style)\n",
    "            loss = fruit_loss + style_loss\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=(fruit_logits, style_logits),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = 5\n",
    "learning_rate = 5e-4\n",
    "batch_size = 128\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fruit = len(set(int(x) for x in dataset[\"fruit\"]))\n",
    "num_style = len(set(int(x) for x in dataset[\"style\"]))\n",
    "\n",
    "print(\"num_fruit: \", num_fruit)\n",
    "print(\"num_style: \", num_style)\n",
    "\n",
    "base_config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "config = MultiTaskViTConfig(\n",
    "    **base_config.to_dict(),\n",
    "    num_fruit=num_fruit,\n",
    "    num_style=num_style\n",
    ")\n",
    "\n",
    "model = MultiTaskViT(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit_fruit_classification\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=train_epoch,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./logs',\n",
    "    report_to=[\"tensorboard\"],\n",
    "    label_names=[\"fruit\", \"style\"],\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    fruit_logits, style_logits = eval_pred.predictions\n",
    "\n",
    "    fruit_labels, style_labels = eval_pred.label_ids\n",
    "\n",
    "    fruit_predictions = np.argmax(fruit_logits, axis=-1)\n",
    "    style_predictions = np.argmax(style_logits, axis=-1)\n",
    "\n",
    "    fruit_precision, fruit_recall, fruit_f1, _ = precision_recall_fscore_support(fruit_labels, fruit_predictions,\n",
    "                                                               average='weighted')\n",
    "    fruit_acc = accuracy_score(fruit_labels, fruit_predictions)\n",
    "\n",
    "    style_precision, style_recall, style_f1, _ = precision_recall_fscore_support(style_labels, style_predictions,\n",
    "                                                               average='weighted')\n",
    "    style_acc = accuracy_score(style_labels, style_predictions)\n",
    "\n",
    "    return {\"fruit_acc\": fruit_acc, \"fruit_precision\": fruit_precision, \"fruit_recall\": fruit_recall, \"fruit_f1\": fruit_f1,\n",
    "            \"style_acc\": style_acc, \"style_precision\": style_precision, \"style_recall\": style_recall, \"style_f1\": style_f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./vit_fruit_cls\")\n",
    "\n",
    "feature_extractor.save_pretrained('./vit_fruit_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained('./vit_fruit_cls')\n",
    "model = MultiTaskViT.from_pretrained(\"./vit_fruit_cls\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "fruit_preds, style_preds = [], []\n",
    "fruit_labels, style_labels = [], []\n",
    "\n",
    "for item in tqdm(test):\n",
    "    image = TF.to_pil_image(item['image'])\n",
    "\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        fruit_logits, style_logits = outputs.logits\n",
    "\n",
    "        fruit_pred = torch.argmax(fruit_logits, dim=-1).item()\n",
    "        style_pred = torch.argmax(style_logits, dim=-1).item()\n",
    "\n",
    "    fruit_preds.append(fruit_pred)\n",
    "    style_preds.append(style_pred)\n",
    "    fruit_labels.append(item['fruit'].item())\n",
    "    style_labels.append(item['style'].item())\n",
    "\n",
    "def report(task_name, y_true, y_pred):\n",
    "    print(f\"[{task_name}]\")\n",
    "    print(f\"  Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  Precision: {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"  Recall   : {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"  F1 Score : {f1_score(y_true, y_pred, average='macro'):.4f}\\n\")\n",
    "\n",
    "report(\"Fruit\", fruit_labels, fruit_preds)\n",
    "report(\"Style\", style_labels, style_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement download model and evaluate verify dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai311)",
   "language": "python",
   "name": "ai311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
