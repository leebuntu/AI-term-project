{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard\n",
    "!pip install -qqq accelerate==0.28.0\n",
    "!pip install -qqq transformers==4.48.3\n",
    "!pip install -qqq datasets==3.6.0\n",
    "\n",
    "!pip install -U accelerate\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_file = \"/content/drive/MyDrive/ai_term/train.zip\"\n",
    "extract_dir = \"data\"\n",
    "train_data_labels_path = os.path.join(extract_dir, \"train\", \"train_labels.csv\")\n",
    "\n",
    "if not os.path.exists(extract_dir) or not os.path.exists(train_data_labels_path):\n",
    "    print(f\"{extract_dir} 디렉토리 또는 필요한 파일({train_data_labels_path})이 존재하지 않습니다. 압축 해제합니다.\")\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "    !unzip -q {zip_file} -d {extract_dir}\n",
    "else:\n",
    "    print(f\"{extract_dir} 디렉토리와 필요한 파일이 이미 존재합니다. 압축 해제를 건너뜁니다.\")\n",
    "\n",
    "train_data_labels = train_data_labels_path\n",
    "train_image_path = os.path.join(extract_dir, \"train\", \"images\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_labels = \"../data/train/train_labels.csv\"\n",
    "train_image_path = \"../data/train/images/\"\n",
    "\n",
    "model_output_path = \"./output\"\n",
    "train_output_path = \"./output/train\"\n",
    "\n",
    "true_fruit = [\"apple\", \"asian pear\", \"banana\", \"cherry\", \"grape\", \"pineapple\"]\n",
    "true_style = [\"pencil color\", \"oil painting\", \"water color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import ViTModel, PreTrainedModel, ViTConfig\n",
    "import transformers as tf\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets.features import Image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.mps.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df['image'] = train_image_path + df['file_name']\n",
    "\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds = ds.cast_column(\"image\", Image())\n",
    "\n",
    "    return ds\n",
    "\n",
    "df = pd.read_csv(train_data_labels)\n",
    "\n",
    "dataset = load_data(train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  file_name  style  fruit\n",
      "0     0.jpg      0      0\n",
      "1     1.jpg      0      0\n",
      "2     2.jpg      0      0\n",
      "3     3.jpg      0      0\n",
      "4     4.jpg      0      0\n",
      "\n",
      "Dataset({\n",
      "    features: ['file_name', 'style', 'fruit', 'image'],\n",
      "    num_rows: 7200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7200/7200 [01:29<00:00, 80.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "def preprocess_images(examples):\n",
    "    images = [feature_extractor(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    examples['pixel_values'] = [image['pixel_values'][0] for image in images]\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(preprocess_images, batched=True)\n",
    "dataset.set_format(type='torch', columns=['image', 'pixel_values', 'fruit', 'style'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "train_val = train_val_dataset[\"train\"]\n",
    "test = train_val_dataset[\"test\"]\n",
    "\n",
    "train_val = train_val.train_test_split(test_size=0.2, seed=42)\n",
    "train = train_val[\"train\"]\n",
    "val = train_val[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskViTConfig(ViTConfig):\n",
    "    def __init__(self, num_fruit=6, num_style=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_fruit = num_fruit\n",
    "        self.num_style = num_style\n",
    "\n",
    "class MultiTaskViT(PreTrainedModel):\n",
    "    config_class = MultiTaskViTConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        hidden_size = self.vit.config.hidden_size\n",
    "\n",
    "        self.fruit_classifier = nn.Linear(hidden_size, config.num_fruit)\n",
    "        self.style_classifier = nn.Linear(hidden_size, config.num_style)\n",
    "\n",
    "    def forward(self, pixel_values, fruit=None, style=None, **kwargs):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "\n",
    "        fruit_logits = self.fruit_classifier(pooled_output)\n",
    "        style_logits = self.style_classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if fruit is not None and style is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            fruit_loss = loss_fn(fruit_logits, fruit)\n",
    "            style_loss = loss_fn(style_logits, style)\n",
    "            loss = fruit_loss + style_loss\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=(fruit_logits, style_logits),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = 20\n",
    "learning_rate = 5e-5\n",
    "batch_size = 64\n",
    "weight_decay = 0.01\n",
    "\n",
    "warmup_ratio = 0.1\n",
    "\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_fruit:  6\n",
      "num_style:  3\n"
     ]
    }
   ],
   "source": [
    "num_fruit = len(set(int(x) for x in dataset[\"fruit\"]))\n",
    "num_style = len(set(int(x) for x in dataset[\"style\"]))\n",
    "\n",
    "print(\"num_fruit: \", num_fruit)\n",
    "print(\"num_style: \", num_style)\n",
    "\n",
    "base_config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "config = MultiTaskViTConfig(\n",
    "    **base_config.to_dict(),\n",
    "    num_fruit=num_fruit,\n",
    "    num_style=num_style\n",
    ")\n",
    "\n",
    "model = MultiTaskViT(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=train_output_path,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",                 \n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=train_epoch,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./logs',\n",
    "    report_to=[\"tensorboard\"],\n",
    "    label_names=[\"fruit\", \"style\"],\n",
    "    fp16=True,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    fruit_logits, style_logits = eval_pred.predictions\n",
    "\n",
    "    fruit_labels, style_labels = eval_pred.label_ids\n",
    "\n",
    "    fruit_predictions = np.argmax(fruit_logits, axis=-1)\n",
    "    style_predictions = np.argmax(style_logits, axis=-1)\n",
    "\n",
    "    fruit_precision, fruit_recall, fruit_f1, _ = precision_recall_fscore_support(fruit_labels, fruit_predictions,\n",
    "                                                               average='weighted')\n",
    "    fruit_acc = accuracy_score(fruit_labels, fruit_predictions)\n",
    "\n",
    "    style_precision, style_recall, style_f1, _ = precision_recall_fscore_support(style_labels, style_predictions,\n",
    "                                                               average='weighted')\n",
    "    style_acc = accuracy_score(style_labels, style_predictions)\n",
    "\n",
    "    return {\"fruit_acc\": fruit_acc, \"fruit_precision\": fruit_precision, \"fruit_recall\": fruit_recall, \"fruit_f1\": fruit_f1,\n",
    "            \"style_acc\": style_acc, \"style_precision\": style_precision, \"style_recall\": style_recall, \"style_f1\": style_f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./output/preprocessor_config.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(model_output_path)\n",
    "\n",
    "feature_extractor.save_pretrained(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(model_output_path)\n",
    "model = MultiTaskViT.from_pretrained(model_output_path).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2160, 3, 512, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2160/2160 [01:22<00:00, 26.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fruit]\n",
      "  Accuracy : 0.2611\n",
      "  Precision: 0.2276\n",
      "  Recall   : 0.2562\n",
      "  F1 Score : 0.2292\n",
      "\n",
      "[Style]\n",
      "  Accuracy : 0.3306\n",
      "  Precision: 0.2730\n",
      "  Recall   : 0.3316\n",
      "  F1 Score : 0.2538\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "\n",
    "pred_fruit, pred_style = [], []\n",
    "true_fruit, true_style = [], []\n",
    "\n",
    "for item in tqdm(test):\n",
    "    image = TF.to_pil_image(item['image'])\n",
    "\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        fruit_logits, style_logits = outputs.logits\n",
    "\n",
    "        fruit_pred = torch.argmax(fruit_logits, dim=-1).item()\n",
    "        style_pred = torch.argmax(style_logits, dim=-1).item()\n",
    "\n",
    "    pred_fruit.append(fruit_pred)\n",
    "    pred_style.append(style_pred)\n",
    "    true_fruit.append(item['fruit'].item())\n",
    "    true_style.append(item['style'].item())\n",
    "\n",
    "def report(task_name, y_true, y_pred):\n",
    "    print(f\"[{task_name}]\")\n",
    "    print(f\"  Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  Precision: {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"  Recall   : {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"  F1 Score : {f1_score(y_true, y_pred, average='macro'):.4f}\\n\")\n",
    "\n",
    "report(\"Fruit\", true_fruit, pred_fruit)\n",
    "report(\"Style\", true_style, pred_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_two_confusion_matrix(first_label, first_conf_matrix, first_classes, second_label, second_conf_matrix, second_classes):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    sns.heatmap(first_conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n",
    "                xticklabels=first_classes, yticklabels=first_classes, ax=axes[0])\n",
    "    axes[0].set_title(f\"Confusion Matrix: {first_label}\")\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"Actual\")\n",
    "\n",
    "    sns.heatmap(second_conf_matrix, annot=True, fmt=\"d\", cmap=\"YlOrBr\",\n",
    "                xticklabels=second_classes, yticklabels=second_classes, ax=axes[1])\n",
    "    axes[1].set_title(f\"Confusion Matrix: {second_label}\")\n",
    "    axes[1].set_xlabel(\"Predicted\")\n",
    "    axes[1].set_ylabel(\"Actual\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_classes = sorted(set(dataset[\"fruit\"]))\n",
    "style_classes = sorted(set(dataset[\"style\"]))\n",
    "\n",
    "fruit_conf_matrix = confusion_matrix(true_fruit, pred_fruit)\n",
    "style_conf_matrix = confusion_matrix(true_style, pred_style)\n",
    "\n",
    "draw_two_confusion_matrix(\"Fruit\", fruit_conf_matrix, fruit_classes, \"Style\", style_conf_matrix, style_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai311)",
   "language": "python",
   "name": "ai311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
