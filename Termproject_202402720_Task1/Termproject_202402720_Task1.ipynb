{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (1.72.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (2.2.6)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (6.31.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (78.1.1)\n",
      "Requirement already satisfied: six>1.9 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "zsh:1: no matches found: https://docs.googㅁle.com/uc?id=1e7P8XjrkPSKzIrmjt-zi5ndKxpuG07X8\n",
      "mkdir: data: File exists\n",
      "unzip:  cannot find or open train.zip, train.zip.zip or train.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard\n",
    "!pip install -qqq accelerate==0.28.0\n",
    "!pip install -qqq transformers==4.48.3\n",
    "!pip install -qqq datasets==3.6.0\n",
    "\n",
    "!pip install -U accelerate\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_file = \"/content/drive/MyDrive/ai_term/train.zip\"\n",
    "extract_dir = \"data\"\n",
    "train_data_labels_path = os.path.join(extract_dir, \"train\", \"train_labels.csv\") \n",
    "\n",
    "if not os.path.exists(extract_dir) or not os.path.exists(train_data_labels_path):\n",
    "    print(f\"{extract_dir} 디렉토리 또는 필요한 파일({train_data_labels_path})이 존재하지 않습니다. 압축 해제합니다.\")\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "    !unzip -q {zip_file} -d {extract_dir}\n",
    "else:\n",
    "    print(f\"{extract_dir} 디렉토리와 필요한 파일이 이미 존재합니다. 압축 해제를 건너뜁니다.\")\n",
    "\n",
    "train_data_labels = train_data_labels_path\n",
    "train_image_path = os.path.join(extract_dir, \"train\", \"images\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_labels = \"./data/train/train_labels.csv\"\n",
    "train_image_path = \"./data/train/images/\"\n",
    "\n",
    "model_output_path = \"./output\"\n",
    "\n",
    "fruit_labels = [\"apple\", \"asian pear\", \"banana\", \"cherry\", \"grape\", \"pineapple\"]\n",
    "style_labels = [\"pencil color\", \"oil painting\", \"water color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import ViTModel, PreTrainedModel, ViTConfig\n",
    "import transformers as tf\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets.features import ClassLabel, Image\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)  \n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df['image'] = train_image_path + df['file_name']\n",
    "\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds = ds.cast_column(\"image\", Image())\n",
    "\n",
    "    return ds\n",
    "\n",
    "df = pd.read_csv(train_data_labels)\n",
    "\n",
    "dataset = load_data(train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  file_name  style  fruit\n",
      "0     0.jpg      0      0\n",
      "1     1.jpg      0      0\n",
      "2     2.jpg      0      0\n",
      "3     3.jpg      0      0\n",
      "4     4.jpg      0      0\n",
      "\n",
      "Dataset({\n",
      "    features: ['file_name', 'style', 'fruit', 'image'],\n",
      "    num_rows: 7200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b49996e26f42b8816787be333f0937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extractor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "def preprocess_images(examples):\n",
    "    images = [feature_extractor(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    examples['pixel_values'] = [image['pixel_values'][0] for image in images]\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(preprocess_images, batched=True)\n",
    "dataset.set_format(type='torch', columns=['image', 'pixel_values', 'fruit', 'style'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "train_val = train_val_dataset[\"train\"]\n",
    "test = train_val_dataset[\"test\"]\n",
    "\n",
    "train_val = train_val.train_test_split(test_size=0.2, seed=42)\n",
    "train = train_val[\"train\"]\n",
    "val = train_val[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskViTConfig(ViTConfig):\n",
    "    def __init__(self, num_fruit=6, num_style=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_fruit = num_fruit\n",
    "        self.num_style = num_style\n",
    "\n",
    "class MultiTaskViT(PreTrainedModel):\n",
    "    config_class = MultiTaskViTConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        hidden_size = self.vit.config.hidden_size\n",
    "\n",
    "        self.fruit_classifier = nn.Linear(hidden_size, config.num_fruit)\n",
    "        self.style_classifier = nn.Linear(hidden_size, config.num_style)\n",
    "\n",
    "    def forward(self, pixel_values, fruit=None, style=None, **kwargs):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output \n",
    "\n",
    "\n",
    "        fruit_logits = self.fruit_classifier(pooled_output)\n",
    "        style_logits = self.style_classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if fruit is not None and style is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            fruit_loss = loss_fn(fruit_logits, fruit)\n",
    "            style_loss = loss_fn(style_logits, style)\n",
    "            loss = fruit_loss + style_loss\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=(fruit_logits, style_logits),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = 15\n",
    "learning_rate = 1e-4\n",
    "batch_size = 128\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_fruit:  6\n",
      "num_style:  3\n"
     ]
    }
   ],
   "source": [
    "num_fruit = len(set(int(x) for x in dataset[\"fruit\"]))\n",
    "num_style = len(set(int(x) for x in dataset[\"style\"]))\n",
    "\n",
    "print(\"num_fruit: \", num_fruit)\n",
    "print(\"num_style: \", num_style)\n",
    "\n",
    "config = MultiTaskViTConfig.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    ")\n",
    "\n",
    "model = MultiTaskViT(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit_fruit_classification\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=train_epoch,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./logs',\n",
    "    report_to=[\"tensorboard\"],\n",
    "    label_names=[\"fruit\", \"style\"],\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    fruit_logits, style_logits = eval_pred.predictions\n",
    "\n",
    "    fruit_labels, style_labels = eval_pred.label_ids\n",
    "\n",
    "    fruit_predictions = np.argmax(fruit_logits, axis=-1)\n",
    "    style_predictions = np.argmax(style_logits, axis=-1)\n",
    "\n",
    "    fruit_precision, fruit_recall, fruit_f1, _ = precision_recall_fscore_support(fruit_labels, fruit_predictions,\n",
    "                                                               average='weighted')\n",
    "    fruit_acc = accuracy_score(fruit_labels, fruit_predictions)\n",
    "\n",
    "    style_precision, style_recall, style_f1, _ = precision_recall_fscore_support(style_labels, style_predictions,\n",
    "                                                               average='weighted')\n",
    "    style_acc = accuracy_score(style_labels, style_predictions)\n",
    "\n",
    "    return {\"fruit_acc\": fruit_acc, \"fruit_precision\": fruit_precision, \"fruit_recall\": fruit_recall, \"fruit_f1\": fruit_f1,\n",
    "            \"style_acc\": style_acc, \"style_precision\": style_precision, \"style_recall\": style_recall, \"style_f1\": style_f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Fruit Acc</th>\n",
       "      <th>Fruit Precision</th>\n",
       "      <th>Fruit Recall</th>\n",
       "      <th>Fruit F1</th>\n",
       "      <th>Style Acc</th>\n",
       "      <th>Style Precision</th>\n",
       "      <th>Style Recall</th>\n",
       "      <th>Style F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.129347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.031596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.4414660135904948, metrics={'train_runtime': 7.7772, 'train_samples_per_second': 30.859, 'train_steps_per_second': 1.929, 'total_flos': 0.0, 'train_loss': 0.4414660135904948, 'epoch': 3.0})"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "File ./vit_fruit_cls cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[287]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./vit_fruit_cls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m feature_extractor.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./vit_fruit_cls\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages/torch/serialization.py:943\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    940\u001b[39m _check_save_filelike(f)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    944\u001b[39m         _save(\n\u001b[32m    945\u001b[39m             obj,\n\u001b[32m    946\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m             _disable_byteorder_record,\n\u001b[32m    950\u001b[39m         )\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages/torch/serialization.py:810\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    809\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages/torch/serialization.py:781\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    777\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    778\u001b[39m         torch._C.PyTorchFileWriter(\u001b[38;5;28mself\u001b[39m.file_stream, _compute_crc32)\n\u001b[32m    779\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compute_crc32\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: File ./vit_fruit_cls cannot be opened."
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./vit_fruit_cls\")\n",
    "\n",
    "feature_extractor.save_pretrained('./vit_fruit_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ai311/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained('./vit_fruit_cls')\n",
    "model = MultiTaskViT.from_pretrained(\"./vit_fruit_cls\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3,  ..., 5, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1440/1440 [00:49<00:00, 29.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fruit]\n",
      "  Accuracy : 0.9993\n",
      "  Precision: 0.9993\n",
      "  Recall   : 0.9993\n",
      "  F1 Score : 0.9993\n",
      "\n",
      "[Style]\n",
      "  Accuracy : 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall   : 1.0000\n",
      "  F1 Score : 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "fruit_preds, style_preds = [], []\n",
    "fruit_labels, style_labels = [], []\n",
    "\n",
    "for item in tqdm(test):\n",
    "    image = TF.to_pil_image(item['image'])\n",
    "\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        fruit_logits, style_logits = outputs.logits\n",
    "\n",
    "        fruit_pred = torch.argmax(fruit_logits, dim=-1).item()\n",
    "        style_pred = torch.argmax(style_logits, dim=-1).item()\n",
    "\n",
    "    fruit_preds.append(fruit_pred)\n",
    "    style_preds.append(style_pred)\n",
    "    fruit_labels.append(item['fruit'].item())\n",
    "    style_labels.append(item['style'].item())\n",
    "\n",
    "def report(task_name, y_true, y_pred):\n",
    "    print(f\"[{task_name}]\")\n",
    "    print(f\"  Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  Precision: {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"  Recall   : {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"  F1 Score : {f1_score(y_true, y_pred, average='macro'):.4f}\\n\")\n",
    "\n",
    "report(\"Fruit\", fruit_labels, fruit_preds)\n",
    "report(\"Style\", style_labels, style_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai311)",
   "language": "python",
   "name": "ai311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
